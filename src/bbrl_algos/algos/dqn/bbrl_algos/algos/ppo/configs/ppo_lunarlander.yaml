    save_best: True
    plot_agents: False
    collect_stats: True

    logger:
      classname: bbrl.utils.logger.TFLogger
      log_dir: ./ppo_logs/
      verbose: False
      every_n_seconds: 10

    algorithm:
      seed:
        train: 335
        eval: 983
        policy: 123
        torch: 789

      max_grad_norm: 0.5
      n_envs: 10
      n_steps_train: 50
      eval_interval: 2000
      nb_evals: 10
      gae: 0.9
      n_steps: 300_000
      beta: 5.
      discount_factor: 0.95
      clip_range: 0.2
      clip_range_vf: 0
      entropy_coef: 2e-7
      critic_coef: 0.4
      policy_coef: 1
      opt_epochs: 3
      batch_size: 16
      policy_type: DiscreteActor
      architecture:
        policy_hidden_size: [256, 256]
        critic_hidden_size: [256, 256]

    gym_env:
      env_name: LunarLander-v2

    optimizer:
      classname: torch.optim.Adam
      lr: 0.0003
