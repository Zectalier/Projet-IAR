{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Machine_Learning\\Anaconda\\envs\\IAR\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mf:\\Documents\\Master S2\\IAR\\Projet-IAR\\src\\Untitled-1.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Documents/Master%20S2/IAR/Projet-IAR/src/Untitled-1.ipynb#W1sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m observations\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Documents/Master%20S2/IAR/Projet-IAR/src/Untitled-1.ipynb#W1sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Create two DQN models, one for the policy network and another for the target network\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/Documents/Master%20S2/IAR/Projet-IAR/src/Untitled-1.ipynb#W1sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m policy_model \u001b[39m=\u001b[39m DQN(\u001b[39m\"\u001b[39;49m\u001b[39mMlpPolicy\u001b[39;49m\u001b[39m\"\u001b[39;49m, env, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, policy_kwargs\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mfeatures_extractor_class\u001b[39;49m\u001b[39m\"\u001b[39;49m: CustomFeatureExtractor})\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Documents/Master%20S2/IAR/Projet-IAR/src/Untitled-1.ipynb#W1sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m target_model \u001b[39m=\u001b[39m DQN(\u001b[39m\"\u001b[39m\u001b[39mMlpPolicy\u001b[39m\u001b[39m\"\u001b[39m, env, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, policy_kwargs\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mfeatures_extractor_class\u001b[39m\u001b[39m\"\u001b[39m: CustomFeatureExtractor})\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Documents/Master%20S2/IAR/Projet-IAR/src/Untitled-1.ipynb#W1sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Define a custom loss function for the critic network (target network)\u001b[39;00m\n",
      "File \u001b[1;32mf:\\Machine_Learning\\Anaconda\\envs\\IAR\\lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:141\u001b[0m, in \u001b[0;36mDQN.__init__\u001b[1;34m(self, policy, env, learning_rate, buffer_size, learning_starts, batch_size, tau, gamma, train_freq, gradient_steps, replay_buffer_class, replay_buffer_kwargs, optimize_memory_usage, target_update_interval, exploration_fraction, exploration_initial_eps, exploration_final_eps, max_grad_norm, stats_window_size, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexploration_rate \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m    140\u001b[0m \u001b[39mif\u001b[39;00m _init_setup_model:\n\u001b[1;32m--> 141\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setup_model()\n",
      "File \u001b[1;32mf:\\Machine_Learning\\Anaconda\\envs\\IAR\\lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:144\u001b[0m, in \u001b[0;36mDQN._setup_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_setup_model\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 144\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_setup_model()\n\u001b[0;32m    145\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_aliases()\n\u001b[0;32m    146\u001b[0m     \u001b[39m# Copy running stats, see GH issue #996\u001b[39;00m\n",
      "File \u001b[1;32mf:\\Machine_Learning\\Anaconda\\envs\\IAR\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:199\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm._setup_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    188\u001b[0m         replay_buffer_kwargs[\u001b[39m\"\u001b[39m\u001b[39menv\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\n\u001b[0;32m    189\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplay_buffer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplay_buffer_class(\n\u001b[0;32m    190\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer_size,\n\u001b[0;32m    191\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mreplay_buffer_kwargs,  \u001b[39m# pytype:disable=wrong-keyword-args\u001b[39;00m\n\u001b[0;32m    197\u001b[0m     )\n\u001b[1;32m--> 199\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_class(  \u001b[39m# pytype:disable=not-instantiable\u001b[39;00m\n\u001b[0;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space,\n\u001b[0;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space,\n\u001b[0;32m    202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr_schedule,\n\u001b[0;32m    203\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_kwargs,  \u001b[39m# pytype:disable=not-instantiable\u001b[39;00m\n\u001b[0;32m    204\u001b[0m )\n\u001b[0;32m    205\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    207\u001b[0m \u001b[39m# Convert train freq parameter to TrainFreq object\u001b[39;00m\n",
      "File \u001b[1;32mf:\\Machine_Learning\\Anaconda\\envs\\IAR\\lib\\site-packages\\stable_baselines3\\dqn\\policies.py:151\u001b[0m, in \u001b[0;36mDQNPolicy.__init__\u001b[1;34m(self, observation_space, action_space, lr_schedule, net_arch, activation_fn, features_extractor_class, features_extractor_kwargs, normalize_images, optimizer_class, optimizer_kwargs)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation_fn \u001b[39m=\u001b[39m activation_fn\n\u001b[0;32m    143\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet_args \u001b[39m=\u001b[39m {\n\u001b[0;32m    144\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mobservation_space\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space,\n\u001b[0;32m    145\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39maction_space\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mnormalize_images\u001b[39m\u001b[39m\"\u001b[39m: normalize_images,\n\u001b[0;32m    149\u001b[0m }\n\u001b[1;32m--> 151\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build(lr_schedule)\n",
      "File \u001b[1;32mf:\\Machine_Learning\\Anaconda\\envs\\IAR\\lib\\site-packages\\stable_baselines3\\dqn\\policies.py:163\u001b[0m, in \u001b[0;36mDQNPolicy._build\u001b[1;34m(self, lr_schedule)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_build\u001b[39m(\u001b[39mself\u001b[39m, lr_schedule: Schedule) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[39m    Create the network and the optimizer.\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[39m        lr_schedule(1) is the initial learning rate\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 163\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_net \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmake_q_net()\n\u001b[0;32m    164\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_net_target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_q_net()\n\u001b[0;32m    165\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_net_target\u001b[39m.\u001b[39mload_state_dict(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_net\u001b[39m.\u001b[39mstate_dict())\n",
      "File \u001b[1;32mf:\\Machine_Learning\\Anaconda\\envs\\IAR\\lib\\site-packages\\stable_baselines3\\dqn\\policies.py:177\u001b[0m, in \u001b[0;36mDQNPolicy.make_q_net\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_q_net\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m QNetwork:\n\u001b[0;32m    176\u001b[0m     \u001b[39m# Make sure we always have separate networks for features extractors etc\u001b[39;00m\n\u001b[1;32m--> 177\u001b[0m     net_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_features_extractor(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnet_args, features_extractor\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m    178\u001b[0m     \u001b[39mreturn\u001b[39;00m QNetwork(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mnet_args)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[1;32mf:\\Machine_Learning\\Anaconda\\envs\\IAR\\lib\\site-packages\\stable_baselines3\\common\\policies.py:114\u001b[0m, in \u001b[0;36mBaseModel._update_features_extractor\u001b[1;34m(self, net_kwargs, features_extractor)\u001b[0m\n\u001b[0;32m    111\u001b[0m net_kwargs \u001b[39m=\u001b[39m net_kwargs\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m    112\u001b[0m \u001b[39mif\u001b[39;00m features_extractor \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    113\u001b[0m     \u001b[39m# The features extractor is not shared, create a new one\u001b[39;00m\n\u001b[1;32m--> 114\u001b[0m     features_extractor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmake_features_extractor()\n\u001b[0;32m    115\u001b[0m net_kwargs\u001b[39m.\u001b[39mupdate(\u001b[39mdict\u001b[39m(features_extractor\u001b[39m=\u001b[39mfeatures_extractor, features_dim\u001b[39m=\u001b[39mfeatures_extractor\u001b[39m.\u001b[39mfeatures_dim))\n\u001b[0;32m    116\u001b[0m \u001b[39mreturn\u001b[39;00m net_kwargs\n",
      "File \u001b[1;32mf:\\Machine_Learning\\Anaconda\\envs\\IAR\\lib\\site-packages\\stable_baselines3\\common\\policies.py:120\u001b[0m, in \u001b[0;36mBaseModel.make_features_extractor\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_features_extractor\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BaseFeaturesExtractor:\n\u001b[0;32m    119\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Helper method to create a features extractor.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures_extractor_class(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures_extractor_kwargs)\n",
      "File \u001b[1;32mf:\\Machine_Learning\\Anaconda\\envs\\IAR\\lib\\site-packages\\stable_baselines3\\common\\torch_layers.py:23\u001b[0m, in \u001b[0;36mBaseFeaturesExtractor.__init__\u001b[1;34m(self, observation_space, features_dim)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, observation_space: gym\u001b[39m.\u001b[39mSpace, features_dim: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m---> 23\u001b[0m     \u001b[39massert\u001b[39;00m features_dim \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     24\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_observation_space \u001b[39m=\u001b[39m observation_space\n\u001b[0;32m     25\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_features_dim \u001b[39m=\u001b[39m features_dim\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from stable_baselines3 import DQN\n",
    "import torch\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "# Define a custom feature extractor for the policy and target models\n",
    "class CustomFeatureExtractor(BaseFeaturesExtractor):\n",
    "    def forward(self, observations):\n",
    "        # Assuming observations are already normalized\n",
    "        return observations\n",
    "\n",
    "# Create two DQN models, one for the policy network and another for the target network\n",
    "policy_model = DQN(\"MlpPolicy\", env, verbose=1, policy_kwargs={\"features_extractor_class\": CustomFeatureExtractor})\n",
    "target_model = DQN(\"MlpPolicy\", env, verbose=1, policy_kwargs={\"features_extractor_class\": CustomFeatureExtractor})\n",
    "\n",
    "# Define a custom loss function for the critic network (target network)\n",
    "def double_q_loss(policy_q, target_q1, target_q2):\n",
    "    return torch.mean((policy_q - torch.min(target_q1, target_q2)) ** 2)\n",
    "\n",
    "# Set the loss function for the critic network in the policy model\n",
    "policy_model.policy.net.critic.loss_fn = double_q_loss\n",
    "\n",
    "# Train the DDQN model\n",
    "total_timesteps = 10000  # Adjust this as needed\n",
    "target_update_interval = 100  # Adjust this as needed\n",
    "\n",
    "for t in range(total_timesteps):\n",
    "    action, _ = policy_model.predict(obs, deterministic=True)\n",
    "    new_obs, reward, done, info = env.step(action)\n",
    "\n",
    "    # Update the policy network\n",
    "    policy_model.learn(total_timesteps=1)\n",
    "\n",
    "    if t % target_update_interval == 0:\n",
    "        # Update the target network\n",
    "        target_model.load_state_dict(policy_model.state_dict())\n",
    "\n",
    "    obs = new_obs\n",
    "\n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "\n",
    "# Save and load the DDQN model\n",
    "policy_model.save(\"DDQN-LunarLander-v2\")\n",
    "del policy_model\n",
    "\n",
    "policy_model = DQN.load(\"DDQN-LunarLander-v2\")\n",
    "\n",
    "# Evaluate the policy\n",
    "mean_reward, std_reward = evaluate_policy(policy_model, env, n_eval_episodes=10)\n",
    "print(f\"Mean Reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IAR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
