save_best: True
plot_agents: False
collect_stats: True
    
logger:
  classname: bbrl.utils.logger.TFLogger
  log_dir: ./ppo_logs/
  verbose: False
  every_n_seconds: 10

algorithm:
  seed:
    train: 335
    eval: 983
    policy: 123
    torch: 789

  max_grad_norm: 0.5
  n_envs: 8
  n_steps_train: 20
  n_steps: 70000
  eval_interval: 2000
  nb_evals: 10
  gae: 0.9
  opt_epochs: 3
  batch_size: 16
  beta: 0.1
  discount_factor: 0.95
  clip_range: 0.2
  clip_range_vf: 0
  entropy_coef: 2e-7
  critic_coef: 0.4
  policy_coef: 1
  policy_type: ConstantVarianceContinuousActor
  architecture:
    policy_hidden_size: [256, 256]
    critic_hidden_size: [256, 256]

gym_env:
  env_name: LunarLanderContinuous-v2

optimizer:
  classname: torch.optim.Adam
  lr: 0.0003
