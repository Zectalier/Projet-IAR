      save_best: False
      plot_agents: False
      collect_stats: True

      logger:
            classname: bbrl.utils.logger.TFLogger
            log_dir: ./ppo_logs/
            verbose: False
            every_n_seconds: 10

      algorithm:
            seed:
                  train: 335
                  eval: 983
                  policy: 123
                  torch: 789

            max_grad_norm: 0.5
            n_envs: 1
            n_steps_train: 20
            n_steps: 70000
            eval_interval: 1000
            nb_evals: 1
            gae: 0.95
            discount_factor: 0.9
            clip_range: 0.2
            clip_range_vf: 0
            entropy_coef: 2e-7
            critic_coef: 0.4
            policy_coef: 1
            opt_epochs: 10
            batch_size: 50
            beta: 0.0
            policy_type: TunableVariancePPOActor
            architecture:
                  policy_hidden_size: [64, 64]
                  critic_hidden_size: [64, 64]

      gym_env:
            env_name: SingleStateMDP-v0

      optimizer:
            classname: torch.optim.Adam
            lr: 0.001
