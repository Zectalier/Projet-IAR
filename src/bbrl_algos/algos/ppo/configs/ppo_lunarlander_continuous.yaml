    save_best: True
    plot_agents: False
    collect_stats: True
    
    logger:
      classname: bbrl.utils.logger.TFLogger
      log_dir: ./ppo_logs/
      verbose: False
      every_n_seconds: 10

    algorithm:
      seed:
          train: 335
          eval: 983
          policy: 123
          torch: 789

      max_grad_norm: 0.5
      n_envs: 8
      n_steps_train: 20
      n_steps: 70000
      eval_interval: 2000
      nb_evals: 10
      gae: 0.9
      opt_epochs: 3
      batch_size: 16
      beta: 0.1
      discount_factor: 0.95
      clip_range: 0.2
      clip_range_vf: 0
      entropy_coef: 2e-7
      critic_coef: 0.4
      policy_coef: 1
      policy_type: ConstantVarianceContinuousActor
      architecture:
        policy_hidden_size: [256, 256]
        critic_hidden_size: [256, 256]

    gym_env:
      env_name: LunarLanderContinuous-v2

    optimizer:
      classname: torch.optim.Adam
      lr: 0.0003
